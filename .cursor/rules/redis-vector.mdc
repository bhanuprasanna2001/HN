---
globs: backend/app/services/redis*.py, pipelines/**/*.py
---

# Redis Vector Store Rules

## Redis Stack (not plain Redis)
- Use **Redis Stack** (includes RediSearch + RedisJSON) for vector search capabilities
- Docker image: `redis/redis-stack:latest`
- Connection via `redis.asyncio` in the backend, `redis` (sync) in pipelines

## Index Schema Design

### KB Articles Index (`idx:kb_articles`)
```
Fields:
  - kb_article_id: TAG (filterable, exact match)
  - title: TEXT (full-text searchable)
  - body: TEXT (full-text searchable)
  - tags: TAG (filterable)
  - module: TAG (filterable)
  - category: TAG (filterable)
  - source_type: TAG (filterable — "seed" vs "generated")
  - status: TAG (filterable — "Published", "Draft", "Archived")
  - embedding: VECTOR (FLAT or HNSW, dim depends on model)
  - created_at: NUMERIC (sortable)
  - updated_at: NUMERIC (sortable)
```

### Scripts Index (`idx:scripts`)
```
Fields:
  - script_id: TAG (filterable)
  - script_title: TEXT (full-text searchable)
  - script_purpose: TEXT (full-text searchable)
  - script_inputs: TEXT (placeholder list)
  - module: TAG (filterable)
  - category: TAG (filterable)
  - embedding: VECTOR (FLAT or HNSW)
```

### Conversation Cache (`cache:conversations:{session_id}`)
- Store active conversation state as JSON
- TTL: 30 minutes (configurable)
- Used for multi-turn context in the agent workflow

### Review Queue (`queue:review`)
- Redis Stream or List for pending HITL review items
- Items contain: learning_event_id, kb_draft_id, priority, timestamp

## Embedding Configuration
- **Model**: Use sentence-transformers `all-MiniLM-L6-v2` (384 dim) for local/fast, or OpenAI `text-embedding-3-small` (1536 dim) for quality
- **Dimension**: Must match the model output — configure in env vars
- **Distance metric**: COSINE (default for text similarity)
- **Index algorithm**: HNSW for production (better recall), FLAT for dev/small datasets

## Query Patterns
- **Hybrid search**: Combine vector similarity with metadata filters
  ```
  FT.SEARCH idx:kb_articles
    "(@category:{Certifications} @status:{Published})=>[KNN 5 @embedding $query_vec AS score]"
  ```
- Always filter by `status: Published` for user-facing queries
- Include `score` in results for confidence display and drift monitoring
- Log every query + scores as OTel span attributes

## Data Loading (Pipeline Phase)
- PySpark processes the Excel → writes JSON Lines files
- A loader script reads JSON Lines → generates embeddings → calls `FT.ADD` / `JSON.SET`
- Batch loading: use Redis pipeline (not individual commands) for bulk insert
- After loading, verify index with `FT.INFO` to confirm document count

## Cache Strategy
- Conversation context: Redis JSON with TTL
- Embedding cache: Cache embeddings for frequently asked questions (TTL: 1 hour)
- Retrieval result cache: Short TTL (5 min) keyed by question hash
- Never cache policy-violating results

## Monitoring
- Track: index size, query latency (p50/p95/p99), hit rate, cache hit rate
- Expose as Prometheus metrics via the backend `/metrics` endpoint
- Alert if retrieval latency p95 > 500ms
