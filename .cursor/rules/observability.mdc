---
globs: backend/app/telemetry/**/*.py, infra/**/*.{yml,yaml}
---

# Observability — OpenTelemetry + Prometheus + Grafana Rules

## Architecture
```
                    ┌─── Traces ───► stdout / Jaeger (dev)
                    │
FastAPI App ──(OTLP gRPC)──► OTel Collector ──┤
                    │                          │
                    │               Collector exposes :8889/metrics
                    │               (Prometheus exporter endpoint)
                    │                          │
                    │              Prometheus ◄─┘  (scrapes Collector :8889
                    │                              AND scrapes Backend :8000/metrics)
                    │                          │
                    │                    Grafana (reads from Prometheus)
                    │
FastAPI App ──(/metrics)──► Prometheus (direct scrape, prometheus_client)

Evidently ── (separate background job) ──► computes drift/RAG metrics
         ── outputs ──► JSON/HTML reports (artifact)
                    ──► exposes /drift-metrics (Prometheus scrapes)
```

### Key dataflow rules:
- **Prometheus scrapes** — it does NOT receive pushes (except via remote_write or pushgateway)
- OTel Collector exposes a **Prometheus exporter endpoint** that Prometheus scrapes
- Backend ALSO exposes `/metrics` directly via `prometheus_client` (belt + suspenders)
- **Evidently is NOT an OTel export target** — it is a standalone service/job that computes drift reports
- Grafana reads ONLY from Prometheus as its data source

## OpenTelemetry Instrumentation

### Auto-Instrumentation
- Use `opentelemetry-instrumentation-fastapi` for automatic HTTP span creation
- Use `opentelemetry-instrumentation-redis` for Redis operation spans
- Use `opentelemetry-instrumentation-httpx` or `opentelemetry-instrumentation-requests` for outgoing LLM API calls

### Custom Instrumentation (Critical — this is the differentiator)
Every LangGraph node must emit:

1. **Span** with:
   - `trustops.node.name` (e.g., "TriageAgent", "RetrievalAgent")
   - `trustops.node.duration_ms`
   - `trustops.node.status` (success/failure/skipped)
   - `trustops.session_id`
   - `trustops.trace_id`

2. **Metrics** (counters/histograms):
   - `trustops_ask_total` — counter of total questions asked
   - `trustops_triage_result` — counter by triage type (KB/SCRIPT/ESCALATE)
   - `trustops_retrieval_latency_ms` — histogram of retrieval time
   - `trustops_retrieval_hit_at_k` — gauge of hit@k accuracy (from eval runs)
   - `trustops_citation_coverage` — gauge (% answers with valid citations)
   - `trustops_policy_violations_total` — counter by violation type
   - `trustops_confidence_distribution` — histogram of answer confidence
   - `trustops_answer_latency_ms` — histogram of end-to-end answer time
   - `trustops_learning_events_total` — counter by status (detected/drafted/approved/rejected)
   - `trustops_kb_articles_total` — gauge of total published KB articles
   - `trustops_drift_alerts_total` — counter of drift alerts triggered

3. **Events** (logged to spans):
   - `policy_violation_detected` with violation details
   - `kb_article_published` with article_id and lineage
   - `learning_event_created` with trigger details
   - `escalation_triggered` with reason

## Prometheus Configuration
- Scrape interval: 15s
- **Scrape targets** (Prometheus pulls from these — all three listed in prometheus.yml):
  1. FastAPI backend `/metrics` endpoint (via `prometheus_client` library)
  2. OTel Collector Prometheus exporter endpoint (`:8889/metrics`)
  3. Evidently drift service `/drift-metrics` endpoint (drift gauges)
- Labels: `service=trustops-backend`, `environment=dev|prod`

## Grafana Dashboards (Pre-built — judges love this)

### Dashboard 1: Trust Overview
- Row 1: Ask volume (rate), Triage distribution (pie), Avg confidence (gauge)
- Row 2: Retrieval hit@k trend (line), Citation coverage (gauge), Policy violations (counter)
- Row 3: E2E latency (histogram), Per-node latency breakdown (stacked bar)

### Dashboard 2: Self-Learning Pipeline
- Row 1: Learning events created (rate), Approval rate (pie), KB growth (line)
- Row 2: Before/after retrieval accuracy (comparison), KB freshness distribution
- Row 3: Drift alerts timeline

### Dashboard 3: Compliance & Safety
- Row 1: Policy violations by type (bar), Escalation rate (gauge)
- Row 2: Red flag detections, PII redaction count
- Row 3: QA score distribution (histogram)

## Drift Detection (Evidently — Standalone Service/Job)
Evidently is NOT part of the OTel pipeline. It is a **separate background service/job** that:
1. Reads collected signals from Redis (rolling window of per-question data)
2. Computes drift reports and RAG evaluation metrics (hit rate, precision@k, MRR)
3. Outputs results in two ways:
   - **Report artifacts**: JSON/HTML drift reports stored in `data/reports/` or served via API
   - **Prometheus metrics**: Evidently drift service exposes a `/drift-metrics` endpoint with drift gauge values. Prometheus scrapes this endpoint alongside the backend `/metrics` endpoint. (Preferred path: **scrape endpoint**, not Pushgateway, because the drift job runs on a recurring schedule and stays alive.)

### Scheduling
- Run as a FastAPI `BackgroundTask` or a separate scheduled job (APScheduler / cron)
- Trigger: every 50 questions OR every 15 minutes (whichever comes first)

### Drift signals monitored
- **Topic drift**: distribution of question categories over time
- **Retrieval score drift**: mean/std of top-k similarity scores dropping
- **Answer type drift**: distribution of KB/SCRIPT/ESCALATE ratio changing
- **Confidence drift**: mean answer confidence trending down
- **RAG quality metrics**: hit rate@k, precision@k, MRR from ground-truth eval runs

### Integration
- Drift alerts emitted as OTel span events + Prometheus counter increments
- Drift reports served via `GET /api/v1/metrics/drift` endpoint (JSON)
- Grafana reads drift metrics from Prometheus like any other metric

## Local Dev Stack
- Use Grafana's `docker-otel-lgtm` Docker image for all-in-one local observability
- Alternative: separate containers in docker-compose (OTel Collector, Prometheus, Grafana)
- Grafana dashboards provisioned via JSON files in `infra/grafana/dashboards/`
- Prometheus config in `infra/prometheus.yml`
- OTel Collector config in `infra/otel-collector-config.yaml`

## Setup in Code
```python
# backend/app/telemetry/setup.py
# Initialize tracer + meter providers in FastAPI lifespan
# Export via OTLP to collector (TRUSTOPS_OTEL_ENDPOINT env var)
# Register FastAPI auto-instrumentation
# Register Redis auto-instrumentation
```
